{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"hw-rnn-wzt.ipynb","provenance":[],"authorship_tag":"ABX9TyNY9UlJ5hP7sXoRboJBtmJU"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwJHtznsXG54","executionInfo":{"status":"ok","timestamp":1611390595653,"user_tz":480,"elapsed":5112,"user":{"displayName":"Zhitao Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8N2z72Fy-h2K97N-5dnjzhEQ-Ak4DhGM8CUry=s64","userId":"11581479042427428761"}},"outputId":"19ec82ad-294b-40a2-ff71-4ef2235c511e"},"source":["!gdown --id '1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8' --output data.zip\r\n","!unzip data.zip\r\n","!ls"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1lz0Wtwxsh5YCPdqQ3E3l_nbfJT1N13V8\n","To: /content/data.zip\n","45.1MB [00:00, 63.0MB/s]\n","Archive:  data.zip\n","  inflating: training_label.txt      \n","  inflating: testing_data.txt        \n","  inflating: training_nolabel.txt    \n","data.zip     testing_data.txt\t training_nolabel.txt\n","sample_data  training_label.txt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jpLEeXqKXPol","executionInfo":{"status":"ok","timestamp":1611393895041,"user_tz":480,"elapsed":929,"user":{"displayName":"Zhitao Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8N2z72Fy-h2K97N-5dnjzhEQ-Ak4DhGM8CUry=s64","userId":"11581479042427428761"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import torch.optim as optim\r\n","from torch.utils import data\r\n","\r\n","import numpy as np\r\n","import pandas as pd\r\n","\r\n","import os\r\n","import argparse\r\n","from gensim.models import word2vec, Word2Vec\r\n","from sklearn.model_selection import train_test_split"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"UkBJthN_Zjpb","executionInfo":{"status":"ok","timestamp":1611391287660,"user_tz":480,"elapsed":1847,"user":{"displayName":"Zhitao Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8N2z72Fy-h2K97N-5dnjzhEQ-Ak4DhGM8CUry=s64","userId":"11581479042427428761"}}},"source":["def load_training_data(path='training_label.txt'):\r\n","    # 把 training 時需要的 data 讀進來\r\n","    # 如果是 'training_label.txt'，需要讀取 label，如果是 'training_nolabel.txt'，不需要讀取 label\r\n","    if 'training_label' in path:\r\n","        with open(path, 'r') as f:\r\n","            lines = f.readlines()\r\n","            lines = [line.strip('\\n').split(' ') for line in lines]\r\n","        x = [line[2:] for line in lines]\r\n","        y = [line[0] for line in lines]\r\n","        return x, y\r\n","    else:\r\n","        with open(path, 'r') as f:\r\n","            lines = f.readlines()\r\n","            x = [line.strip('\\n').split(' ') for line in lines]\r\n","        return x\r\n","\r\n","def load_testing_data(path='testing_data'):\r\n","    # 把 testing 時需要的 data 讀進來\r\n","    with open(path, 'r') as f:\r\n","        lines = f.readlines()\r\n","        X = [\"\".join(line.strip('\\n').split(\",\")[1:]).strip() for line in lines[1:]]\r\n","        X = [sen.split(' ') for sen in X]\r\n","    return X"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"NNWiHUEJZ7n4","executionInfo":{"status":"ok","timestamp":1611391314771,"user_tz":480,"elapsed":1495,"user":{"displayName":"Zhitao Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8N2z72Fy-h2K97N-5dnjzhEQ-Ak4DhGM8CUry=s64","userId":"11581479042427428761"}}},"source":["def train_word2vec(x):\r\n","    # 訓練 word to vector 的 word embedding\r\n","    model = word2vec.Word2Vec(x, size=250, window=5, min_count=5, workers=12, iter=10, sg=1)\r\n","    return model"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"EP_x6x2JjJ9i","executionInfo":{"status":"ok","timestamp":1611393855798,"user_tz":480,"elapsed":896,"user":{"displayName":"Zhitao Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8N2z72Fy-h2K97N-5dnjzhEQ-Ak4DhGM8CUry=s64","userId":"11581479042427428761"}}},"source":["class Preprocess():\r\n","    def __init__(self, sentences, sen_len, w2v_path=\"./w2v.model\"):\r\n","        self.w2v_path = w2v_path\r\n","        self.sentences = sentences\r\n","        self.sen_len = sen_len\r\n","        self.idx2word = []\r\n","        self.word2idx = {}\r\n","        self.embedding_matrix = []\r\n","    def get_w2v_model(self):\r\n","        # 把之前訓練好的 word to vec 模型讀進來\r\n","        self.embedding = Word2Vec.load(self.w2v_path)\r\n","        self.embedding_dim = self.embedding.vector_size\r\n","    def add_embedding(self, word):\r\n","        # 把 word 加進 embedding，並賦予他一個隨機生成的 representation vector\r\n","        # word 只會是 \"<PAD>\" 或 \"<UNK>\"\r\n","        vector = torch.empty(1, self.embedding_dim)\r\n","        torch.nn.init.uniform_(vector)\r\n","        self.word2idx[word] = len(self.word2idx)\r\n","        self.idx2word.append(word)\r\n","        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\r\n","    def make_embedding(self, load=True):\r\n","        print(\"Get embedding ...\")\r\n","        # 取得訓練好的 Word2vec word embedding\r\n","        if load:\r\n","            print(\"loading word to vec model ...\")\r\n","            self.get_w2v_model()\r\n","        else:\r\n","            raise NotImplementedError\r\n","        # 製作一個 word2idx 的 dictionary\r\n","        # 製作一個 idx2word 的 list\r\n","        # 製作一個 word2vector 的 list\r\n","        for i, word in enumerate(self.embedding.wv.vocab):\r\n","            print('get words #{}'.format(i+1), end='\\r')\r\n","            #e.g. self.word2index['he'] = 1 \r\n","            #e.g. self.index2word[1] = 'he'\r\n","            #e.g. self.vectors[1] = 'he' vector\r\n","            self.word2idx[word] = len(self.word2idx)\r\n","            self.idx2word.append(word)\r\n","            self.embedding_matrix.append(self.embedding[word])\r\n","        print('')\r\n","        self.embedding_matrix = torch.tensor(self.embedding_matrix)\r\n","        # 將 \"<PAD>\" 跟 \"<UNK>\" 加進 embedding 裡面\r\n","        self.add_embedding(\"<PAD>\")\r\n","        self.add_embedding(\"<UNK>\")\r\n","        print(\"total words: {}\".format(len(self.embedding_matrix)))\r\n","        print(\"embedding matrix shape: {}\".format(self.embedding_matrix.shape))\r\n","        return self.embedding_matrix\r\n","    def pad_sequence(self, sentence):\r\n","        # 將每個句子變成一樣的長度\r\n","        if len(sentence) > self.sen_len:\r\n","            sentence = sentence[:self.sen_len]\r\n","        else:\r\n","            pad_len = self.sen_len - len(sentence)\r\n","            for _ in range(pad_len):\r\n","                sentence.append(self.word2idx[\"<PAD>\"])\r\n","        assert len(sentence) == self.sen_len\r\n","        return sentence\r\n","    def sentence_word2idx(self):\r\n","        # 把句子裡面的字轉成相對應的 index\r\n","        sentence_list = []\r\n","        for i, sen in enumerate(self.sentences):\r\n","            print('sentence count #{}'.format(i+1), end='\\r')\r\n","            sentence_idx = []\r\n","            for word in sen:\r\n","                if (word in self.word2idx.keys()):\r\n","                    sentence_idx.append(self.word2idx[word])\r\n","                else:\r\n","                    sentence_idx.append(self.word2idx[\"<UNK>\"])\r\n","            # 將每個句子變成一樣的長度\r\n","            sentence_idx = self.pad_sequence(sentence_idx)\r\n","            sentence_list.append(sentence_idx)\r\n","        return torch.LongTensor(sentence_list)\r\n","    def labels_to_tensor(self, y):\r\n","        # 把 labels 轉成 tensor\r\n","        y = [int(label) for label in y]\r\n","        return torch.LongTensor(y)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLB5bgrpjYBA","executionInfo":{"status":"ok","timestamp":1611393858256,"user_tz":480,"elapsed":827,"user":{"displayName":"Zhitao Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8N2z72Fy-h2K97N-5dnjzhEQ-Ak4DhGM8CUry=s64","userId":"11581479042427428761"}}},"source":["class LSTM_Net(nn.Module):\r\n","    def __init__(self, embedding, embedding_dim, hidden_dim, num_layers, dropout=0.5, fix_embedding=True):\r\n","        super(LSTM_Net, self).__init__()\r\n","        # 製作 embedding layer\r\n","        self.embedding = torch.nn.Embedding(embedding.size(0),embedding.size(1))\r\n","        self.embedding.weight = torch.nn.Parameter(embedding)\r\n","        # 是否將 embedding fix 住，如果 fix_embedding 為 False，在訓練過程中，embedding 也會跟著被訓練\r\n","        self.embedding.weight.requires_grad = False if fix_embedding else True\r\n","        self.embedding_dim = embedding.size(1)\r\n","        self.hidden_dim = hidden_dim\r\n","        self.num_layers = num_layers\r\n","        self.dropout = dropout\r\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\r\n","        self.classifier = nn.Sequential( nn.Dropout(dropout),\r\n","                                         nn.Linear(hidden_dim, 1),\r\n","                                         nn.Sigmoid() )\r\n","    def forward(self, inputs):\r\n","        inputs = self.embedding(inputs)\r\n","        x, _ = self.lstm(inputs, None)\r\n","        # x 的 dimension (batch, seq_len, hidden_size)\r\n","        # 取用 LSTM 最後一層的 hidden state\r\n","        x = x[:, -1, :] \r\n","        x = self.classifier(x)\r\n","        return x"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"JhRP-ERMjkv8","executionInfo":{"status":"ok","timestamp":1611393899977,"user_tz":480,"elapsed":945,"user":{"displayName":"Zhitao Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8N2z72Fy-h2K97N-5dnjzhEQ-Ak4DhGM8CUry=s64","userId":"11581479042427428761"}}},"source":["class TwitterDataset(data.Dataset):\r\n","    \"\"\"\r\n","    Expected data shape like:(data_num, data_len)\r\n","    Data can be a list of numpy array or a list of lists\r\n","    input data shape : (data_num, seq_len, feature_dim)\r\n","    \r\n","    __len__ will return the number of data\r\n","    \"\"\"\r\n","    def __init__(self, X, y):\r\n","        self.data = X\r\n","        self.label = y\r\n","    def __getitem__(self, idx):\r\n","        if self.label is None: return self.data[idx]\r\n","        return self.data[idx], self.label[idx]\r\n","    def __len__(self):\r\n","        return len(self.data)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"JBKV-OqdkoTH","executionInfo":{"status":"ok","timestamp":1611394119349,"user_tz":480,"elapsed":1401,"user":{"displayName":"Zhitao Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8N2z72Fy-h2K97N-5dnjzhEQ-Ak4DhGM8CUry=s64","userId":"11581479042427428761"}}},"source":["def training(batch_size, n_epoch, lr, model_dir, train, valid, model, device):\r\n","    total = sum(p.numel() for p in model.parameters())\r\n","    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\r\n","    print('\\nstart training, parameter total:{}, trainable:{}\\n'.format(total, trainable))\r\n","    model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數\r\n","    criterion = nn.BCELoss() # 定義損失函數，這裡我們使用 binary cross entropy loss\r\n","    t_batch = len(train) \r\n","    v_batch = len(valid) \r\n","    optimizer = optim.Adam(model.parameters(), lr=lr) # 將模型的參數給 optimizer，並給予適當的 learning rate\r\n","    total_loss, total_acc, best_acc = 0, 0, 0\r\n","    for epoch in range(n_epoch):\r\n","        total_loss, total_acc = 0, 0\r\n","        # 這段做 training\r\n","        for i, (inputs, labels) in enumerate(train):\r\n","            inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\r\n","            labels = labels.to(device, dtype=torch.float) # device為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\r\n","            optimizer.zero_grad() # 由於 loss.backward() 的 gradient 會累加，所以每次餵完一個 batch 後需要歸零\r\n","            outputs = model(inputs) # 將 input 餵給模型\r\n","            outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\r\n","            loss = criterion(outputs, labels) # 計算此時模型的 training loss\r\n","            loss.backward() # 算 loss 的 gradient\r\n","            optimizer.step() # 更新訓練模型的參數\r\n","            correct = evaluation(outputs, labels) # 計算此時模型的 training accuracy\r\n","            total_acc += (correct / batch_size)\r\n","            total_loss += loss.item()\r\n","            print('[ Epoch{}: {}/{} ] loss:{:.3f} acc:{:.3f} '.format(\r\n","            \tepoch+1, i+1, t_batch, loss.item(), correct*100/batch_size), end='\\r')\r\n","        print('\\nTrain | Loss:{:.5f} Acc: {:.3f}'.format(total_loss/t_batch, total_acc/t_batch*100))\r\n","\r\n","        # 這段做 validation\r\n","        model.eval() # 將 model 的模式設為 eval，這樣 model 的參數就會固定住\r\n","        with torch.no_grad():\r\n","            total_loss, total_acc = 0, 0\r\n","            for i, (inputs, labels) in enumerate(valid):\r\n","                inputs = inputs.to(device, dtype=torch.long) # device 為 \"cuda\"，將 inputs 轉成 torch.cuda.LongTensor\r\n","                labels = labels.to(device, dtype=torch.float) # device 為 \"cuda\"，將 labels 轉成 torch.cuda.FloatTensor，因為等等要餵進 criterion，所以型態要是 float\r\n","                outputs = model(inputs) # 將 input 餵給模型\r\n","                outputs = outputs.squeeze() # 去掉最外面的 dimension，好讓 outputs 可以餵進 criterion()\r\n","                loss = criterion(outputs, labels) # 計算此時模型的 validation loss\r\n","                correct = evaluation(outputs, labels) # 計算此時模型的 validation accuracy\r\n","                total_acc += (correct / batch_size)\r\n","                total_loss += loss.item()\r\n","\r\n","            print(\"Valid | Loss:{:.5f} Acc: {:.3f} \".format(total_loss/v_batch, total_acc/v_batch*100))\r\n","            if total_acc > best_acc:\r\n","                # 如果 validation 的結果優於之前所有的結果，就把當下的模型存下來以備之後做預測時使用\r\n","                best_acc = total_acc\r\n","                #torch.save(model, \"{}/val_acc_{:.3f}.model\".format(model_dir,total_acc/v_batch*100))\r\n","                torch.save(model, \"{}/ckpt.model\".format(model_dir))\r\n","                print('saving model with acc {:.3f}'.format(total_acc/v_batch*100))\r\n","        print('-----------------------------------------------')\r\n","        model.train() # 將 model 的模式設為 train，這樣 optimizer 就可以更新 model 的參數（因為剛剛轉成 eval 模式）\r\n","\r\n","def testing(batch_size, test_loader, model, device):\r\n","    model.eval()\r\n","    ret_output = []\r\n","    with torch.no_grad():\r\n","        for i, inputs in enumerate(test_loader):\r\n","            inputs = inputs.to(device, dtype=torch.long)\r\n","            outputs = model(inputs)\r\n","            outputs = outputs.squeeze()\r\n","            outputs[outputs>=0.5] = 1 # 大於等於 0.5 為正面\r\n","            outputs[outputs<0.5] = 0 # 小於 0.5 為負面\r\n","            ret_output += outputs.int().tolist()\r\n","    \r\n","    return ret_output"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"kwXeki8MaCVM","executionInfo":{"status":"ok","timestamp":1611394582305,"user_tz":480,"elapsed":1518,"user":{"displayName":"Zhitao Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8N2z72Fy-h2K97N-5dnjzhEQ-Ak4DhGM8CUry=s64","userId":"11581479042427428761"}}},"source":["train_with_label = './training_label.txt'\r\n","train_no_label = './training_nolabel.txt'\r\n","test_data = './testing_data.txt'\r\n","\r\n","w2v_path = './w2v.model'\r\n","\r\n","sen_len = 20\r\n","fix_embedding = True\r\n","epoch = 5\r\n","lr = 0.001\r\n","batch_size = 128\r\n","\r\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sodxLA2Tk5I1","executionInfo":{"status":"ok","timestamp":1611394934817,"user_tz":480,"elapsed":344666,"user":{"displayName":"Zhitao Wang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8N2z72Fy-h2K97N-5dnjzhEQ-Ak4DhGM8CUry=s64","userId":"11581479042427428761"}},"outputId":"388de51f-f423-43dc-bca7-5d3634dc5b80"},"source":["print(\"loading training data ...\")\r\n","train_x, y = load_training_data('training_label.txt')\r\n","train_x_no_label = load_training_data('training_nolabel.txt')\r\n","\r\n","print(\"loading testing data ...\")\r\n","test_x = load_testing_data('testing_data.txt')\r\n","\r\n","#model = train_word2vec(train_x + train_x_no_label + test_x)\r\n","model = train_word2vec(train_x + test_x)\r\n","\r\n","print(\"saving model ...\")\r\n","# model.save(os.path.join(path_prefix, 'model/w2v_all.model'))\r\n","model.save(w2v_path)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["loading training data ...\n","loading testing data ...\n","saving model ...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Up9o20lflCX2"},"source":["# 對 input 跟 labels 做預處理\r\n","preprocess = Preprocess(train_x, sen_len, w2v_path=w2v_path)\r\n","embedding = preprocess.make_embedding(load=True)\r\n","train_x = preprocess.sentence_word2idx()\r\n","y = preprocess.labels_to_tensor(y)\r\n","\r\n","# 製作一個 model 的對象\r\n","model = LSTM_Net(embedding, embedding_dim=250, hidden_dim=150, num_layers=1, dropout=0.5, fix_embedding=fix_embedding)\r\n","model = model.to(device) # device為 \"cuda\"，model 使用 GPU 來訓練（餵進去的 inputs 也需要是 cuda tensor）\r\n","\r\n","# 把 data 分為 training data 跟 validation data（將一部份 training data 拿去當作 validation data）\r\n","X_train, X_val, y_train, y_val = train_x[:180000], train_x[180000:], y[:180000], y[180000:]\r\n","\r\n","# 把 data 做成 dataset 供 dataloader 取用\r\n","train_dataset = TwitterDataset(X=X_train, y=y_train)\r\n","val_dataset = TwitterDataset(X=X_val, y=y_val)\r\n","\r\n","# 把 data 轉成 batch of tensors\r\n","train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\r\n","                                            batch_size = batch_size,\r\n","                                            shuffle = True,\r\n","                                            num_workers = 8)\r\n","\r\n","val_loader = torch.utils.data.DataLoader(dataset = val_dataset,\r\n","                                            batch_size = batch_size,\r\n","                                            shuffle = False,\r\n","                                            num_workers = 8)\r\n","\r\n","# 開始訓練\r\n","training(batch_size, epoch, lr, model_dir, train_loader, val_loader, model, device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AttIDGnJlJDv"},"source":["# 開始測試模型並做預測\r\n","print(\"loading testing data ...\")\r\n","test_x = load_testing_data(testing_data)\r\n","preprocess = Preprocess(test_x, sen_len, w2v_path=w2v_path)\r\n","embedding = preprocess.make_embedding(load=True)\r\n","test_x = preprocess.sentence_word2idx()\r\n","test_dataset = TwitterDataset(X=test_x, y=None)\r\n","test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\r\n","                                            batch_size = batch_size,\r\n","                                            shuffle = False,\r\n","                                            num_workers = 8)\r\n","print('\\nload model ...')\r\n","model = torch.load(os.path.join(model_dir, 'ckpt.model'))\r\n","outputs = testing(batch_size, test_loader, model, device)\r\n","\r\n","# 寫到 csv 檔案供上傳 Kaggle\r\n","tmp = pd.DataFrame({\"id\":[str(i) for i in range(len(test_x))],\"label\":outputs})\r\n","print(\"save csv ...\")\r\n","tmp.to_csv(os.path.join(path_prefix, 'predict.csv'), index=False)\r\n","print(\"Finish Predicting\")"],"execution_count":null,"outputs":[]}]}